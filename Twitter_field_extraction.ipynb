{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['contributors', 'coordinates', 'created_at', 'display_text_range',\n",
      "       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',\n",
      "       'favorited', 'filter_level', 'geo', 'id', 'id_str',\n",
      "       'in_reply_to_screen_name', 'in_reply_to_status_id',\n",
      "       'in_reply_to_status_id_str', 'in_reply_to_user_id',\n",
      "       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'limit', 'place',\n",
      "       'possibly_sensitive', 'quote_count', 'quoted_status',\n",
      "       'quoted_status_id', 'quoted_status_id_str', 'reply_count',\n",
      "       'retweet_count', 'retweeted', 'retweeted_status', 'source', 'text',\n",
      "       'timestamp_ms', 'truncated', 'user'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "tweets_data = []\n",
    "tweets_file = open('data_collection12.txt', \"r\") ### this is where i put in the file of collected tweets \n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "tests2=pd.DataFrame(tweets_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "tweets=pd.DataFrame()\n",
    "tweets['createdat']= tests2['created_at']\n",
    "tweets['text']= tests2['text']\n",
    "tweets['geolocation']=tests2['geo']\n",
    "tweets['source']= tests2['source']\n",
    "#tweets['hashtag']=[i['text'] for i in tests2['entities'][0]['hashtags']]\n",
    "hashtags = []\n",
    "for i,j in enumerate(tests2['entities']):\n",
    "    if j is not  np.nan:\n",
    "        if j['hashtags']!=[]:\n",
    "            hashtags.append(j['hashtags'][0]['text'])\n",
    "        else:\n",
    "            hashtags.append(0)\n",
    "    else:\n",
    "        hashtags.append(0)\n",
    "tweets['hashtags']=hashtags\n",
    "name = []\n",
    "id_list=[]\n",
    "\n",
    "for i,j in enumerate(tests2['user']):\n",
    "    if j is not np.nan:\n",
    "        if j['name'] !=[]:\n",
    "            name.append(j['name'])\n",
    "            id_list.append(j['id'])\n",
    "        else:\n",
    "            name.append(0)\n",
    "            id_list.append(0)\n",
    "    else:\n",
    "        name.append(0)\n",
    "        id_list.append(0)\n",
    "        \n",
    "screen_name = []\n",
    "for i,j in enumerate(tests2['user']):\n",
    "    if j is not np.nan:\n",
    "        if j['screen_name'] !=[]:\n",
    "            screen_name.append(j['screen_name'])\n",
    "        else:\n",
    "            screen_name.append(0)\n",
    "    else:\n",
    "        screen_name.append(0)\n",
    "\n",
    "description=[]\n",
    "for i,j in enumerate(tests2['user']):\n",
    "    if j is not np.nan:\n",
    "        if j['description'] !=[]:\n",
    "            description.append(j['description'])\n",
    "        else:\n",
    "            description.append(0)\n",
    "    else:\n",
    "        description.append(0)\n",
    "        \n",
    "followers_count=[]\n",
    "for i,j in enumerate(tests2['user']):\n",
    "    if j is not np.nan:\n",
    "        if j['followers_count'] !=[]:\n",
    "            followers_count.append(j['followers_count'])\n",
    "        else:\n",
    "            followers_count.append(0)\n",
    "    else:\n",
    "        followers_count.append(0)\n",
    "        \n",
    "\n",
    "friends_count=[]\n",
    "for i,j in enumerate(tests2['user']):\n",
    "    if j is not np.nan:\n",
    "        if j['friends_count'] !=[]:\n",
    "            friends_count.append(j['friends_count'])\n",
    "        else:\n",
    "            friends_count.append(0)\n",
    "    else:\n",
    "        friends_count.append(0)\n",
    "        \n",
    "listed_count=[]\n",
    "for i,j in enumerate(tests2['user']):\n",
    "    if j is not np.nan:\n",
    "        if j['listed_count'] !=[]:\n",
    "            listed_count.append(j['listed_count'])\n",
    "        else:\n",
    "            listed_count.append(0)\n",
    "    else:\n",
    "        listed_count.append(0)\n",
    "        \n",
    "favourites_count=[]\n",
    "for i,j in enumerate(tests2['user']):\n",
    "    if j is not np.nan:\n",
    "        if j['favourites_count'] !=[]:\n",
    "            favourites_count.append(j['favourites_count'])\n",
    "        else:\n",
    "            favourites_count.append(0)\n",
    "    else:\n",
    "        favourites_count.append(0)\n",
    "\n",
    "statuses_count=[]\n",
    "for i,j in enumerate(tests2['user']):\n",
    "    if j is not np.nan:\n",
    "        if j['statuses_count'] !=[]:\n",
    "            statuses_count.append(j['statuses_count'])\n",
    "        else:\n",
    "            statuses_count.append(0)\n",
    "    else:\n",
    "        statuses_count.append(0)\n",
    "\n",
    "lang = []\n",
    "for i,j in enumerate(tests2['user']):\n",
    "    if j is not np.nan:\n",
    "        if j['lang'] !=[]:\n",
    "            lang.append(j['lang'])\n",
    "        else:\n",
    "            lang.append(0)\n",
    "    else:\n",
    "        lang.append(0)\n",
    "        \n",
    "mention_list=[]\n",
    "for i in (tests2['entities']):\n",
    "    if i is not np.nan:\n",
    "        mention_list.append([i['user_mentions']])\n",
    "    else:\n",
    "        mention_list.append(0)\n",
    "\n",
    "        \n",
    "tweets['name']=name\n",
    "tweets['id']=id_list\n",
    "tweets['screen_name']=screen_name\n",
    "tweets['description'] = description\n",
    "tweets['followers_count']=followers_count\n",
    "tweets['friends_count'] = friends_count\n",
    "tweets['listed_count']=listed_count\n",
    "tweets['favourites_count']=favourites_count\n",
    "tweets['statuses_count']=statuses_count\n",
    "tweets['language']=lang\n",
    "tweets['mention']=mention_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findWholeWord(w):\n",
    "    return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\n",
    "\n",
    "data2=[]\n",
    "\n",
    "for i,j in enumerate(tweets['text']):\n",
    "    if j is not np.nan:\n",
    "        j = re.sub(\"[^a-zA-Z]\", \" \",j)\n",
    "        if (findWholeWord('by')(j)  ) :\n",
    "        #print(i,j, tweets['lang'].iloc[i],tweets['country'].iloc[i])\n",
    "            data2.append([i,j, tweets['createdat'].iloc[i],\n",
    "                           tweets['geolocation'].iloc[i],\n",
    "                           tweets['source'].iloc[i],\n",
    "                           tweets['hashtags'].iloc[i],\n",
    "                           tweets['name'].iloc[i],\n",
    "                           tweets['id'].iloc[i],\n",
    "                           tweets['screen_name'].iloc[i],\n",
    "                           tweets['description'].iloc[i],\n",
    "                           tweets['followers_count'].iloc[i],\n",
    "                           tweets['friends_count'].iloc[i],\n",
    "                           tweets['listed_count'].iloc[i],\n",
    "                           tweets['favourites_count'].iloc[i],\n",
    "                           tweets['statuses_count'].iloc[i],\n",
    "                           tweets['language'].iloc[i],\n",
    "                           tweets['mention'].iloc[i]]\n",
    "                    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.Series(data2)\n",
    "data_final=[]\n",
    "for i,j in enumerate(data3):\n",
    "    #print(i,j[1])\n",
    "    text = str(j[1])\n",
    "    #print(text)\n",
    "    m = re.search('NowPlaying\\s(.+?)\\sby\\s(.+?)\\son\\s(.+)', text)\n",
    "    if m:\n",
    "        data_final.append([m.group(1),m.group(2),m.group(3),j[1],\n",
    "                         j[2],\n",
    "                         j[3],j[4],j[5],j[6],j[7],j[8],j[9],j[10],j[11],j[12],j[13],j[14],j[15],j[16]])\n",
    "\n",
    "#print(data_final[:10])\n",
    "data_final=pd.DataFrame(data_final)\n",
    "data_final.columns=['song','artist','music_platform','text','createdat',\n",
    "                        'geolocation','source','hashtags','name','id','screen_name','description','followers_count',\n",
    "                        'friends_count','listed_count','favourites_count','statuses_count','language','mention']\n",
    "\n",
    "data_final['source']=data_final['source'].astype(str)\n",
    "data_final['mention']=data_final['mention'].astype(str)\n",
    "#print(data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
